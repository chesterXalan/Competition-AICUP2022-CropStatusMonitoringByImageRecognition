{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b16cf8e9-c11e-4816-8e4d-769f4b01224c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a12e782-34ab-4e70-a6ef-17432f822fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_images: (59031, 480, 480, 3), train_labels: (59031,)\n",
      "valid_images: (25319, 480, 480, 3), valid_labels: (25319,)\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = r'..\\dataset\\npz'\n",
    "training_dataset = np.load(Path(dataset_dir, 'training_cropped_480x480.npz'))\n",
    "train_images = training_dataset['train_images']\n",
    "train_labels = training_dataset['train_labels']\n",
    "valid_images = training_dataset['valid_images']\n",
    "valid_labels = training_dataset['valid_labels']\n",
    "\n",
    "print('train_images: {}, train_labels: {}\\nvalid_images: {}, valid_labels: {}'\n",
    "      .format(train_images.shape, train_labels.shape, valid_images.shape, valid_labels.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66510425-e2fc-423b-9585-80c0c20b73e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = {'ImageNet': transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225]),\n",
    "             'TrainData': transforms.Normalize(mean=[0.431, 0.460, 0.378],\n",
    "                                              std=[0.221, 0.217, 0.242]),\n",
    "             'ValidData': transforms.Normalize(mean=[0.431, 0.459, 0.376],\n",
    "                                               std=[0.221, 0.217, 0.241])}\n",
    "transform = {\n",
    "    'train': nn.Sequential(\n",
    "        transforms.ColorJitter(brightness=.2),\n",
    "        transforms.RandomHorizontalFlip(.2),\n",
    "        transforms.RandomVerticalFlip(.2),\n",
    "        normalize['ImageNet']\n",
    "    ),\n",
    "    'valid': nn.Sequential(\n",
    "        normalize['ImageNet']\n",
    "    )\n",
    "}\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_type, x, y):\n",
    "        self.data_type = data_type\n",
    "        self.image = x\n",
    "        self.label = y\n",
    "        \n",
    "    def __getitem__(self, index): \n",
    "        image = np.transpose(self.image[index], (2, 0, 1))\n",
    "        image = torch.tensor(image, dtype=torch.float32).div_(255).cuda()\n",
    "        image = transform[self.data_type](image)\n",
    "        label = torch.tensor(self.label[index], dtype=torch.int64).cuda()\n",
    "        \n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24916dcf-a62e-4d98-9ac9-e213ce508ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 28\n",
    "train_data = DataLoader(MyDataset('train', train_images, train_labels),\n",
    "                        batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "valid_data = DataLoader(MyDataset('valid', valid_images, valid_labels),\n",
    "                        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dafdc904-435f-4963-8707-f63e03448af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientnetv2_s_20221213_132051\n"
     ]
    }
   ],
   "source": [
    "time_stamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_name = 'efficientnetv2_s_' + time_stamp\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acdfb1d8-1ac5-4f8a-8714-e3a91fff12b1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2dNormActivation(\n",
       "      (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "      )\n",
       "      (1): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.005, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(24, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.01, mode=row)\n",
       "      )\n",
       "      (1): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.015000000000000003, mode=row)\n",
       "      )\n",
       "      (2): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.02, mode=row)\n",
       "      )\n",
       "      (3): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.025, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.030000000000000006, mode=row)\n",
       "      )\n",
       "      (1): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.035, mode=row)\n",
       "      )\n",
       "      (2): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.04, mode=row)\n",
       "      )\n",
       "      (3): FusedMBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.045, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05500000000000001, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06000000000000001, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.065, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07, mode=row)\n",
       "      )\n",
       "      (5): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
       "            (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.075, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
       "            (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.085, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.095, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "      )\n",
       "      (5): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.10500000000000001, mode=row)\n",
       "      )\n",
       "      (6): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.11000000000000001, mode=row)\n",
       "      )\n",
       "      (7): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.11500000000000002, mode=row)\n",
       "      )\n",
       "      (8): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.12000000000000002, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=960, bias=False)\n",
       "            (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.125, mode=row)\n",
       "      )\n",
       "      (1): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.13, mode=row)\n",
       "      )\n",
       "      (2): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.135, mode=row)\n",
       "      )\n",
       "      (3): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14, mode=row)\n",
       "      )\n",
       "      (4): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14500000000000002, mode=row)\n",
       "      )\n",
       "      (5): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15, mode=row)\n",
       "      )\n",
       "      (6): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.155, mode=row)\n",
       "      )\n",
       "      (7): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.16, mode=row)\n",
       "      )\n",
       "      (8): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.165, mode=row)\n",
       "      )\n",
       "      (9): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17, mode=row)\n",
       "      )\n",
       "      (10): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.175, mode=row)\n",
       "      )\n",
       "      (11): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.18, mode=row)\n",
       "      )\n",
       "      (12): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.185, mode=row)\n",
       "      )\n",
       "      (13): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.19, mode=row)\n",
       "      )\n",
       "      (14): MBConv(\n",
       "        (block): Sequential(\n",
       "          (0): Conv2dNormActivation(\n",
       "            (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (1): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
       "            (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (2): SiLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "            (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (activation): SiLU(inplace=True)\n",
       "            (scale_activation): Sigmoid()\n",
       "          )\n",
       "          (3): Conv2dNormActivation(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.195, mode=row)\n",
       "      )\n",
       "    )\n",
       "    (7): Conv2dNormActivation(\n",
       "      (0): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): SiLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=True)\n",
       "    (1): Linear(in_features=1280, out_features=33, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.efficientnet_v2_s(weights='DEFAULT')\n",
    "\n",
    "num_features = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_features, 33)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "    \n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0c8171d-d49e-46d8-8b56-a38f35f4b3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(Path('logs', time_stamp))\n",
    "\n",
    "# drawing model graph\n",
    "model_input, _ = train_data.dataset[0]\n",
    "writer.add_graph(model, model_input.unsqueeze_(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32fd44f7-6dae-49ba-a18c-b19aad8bdbcc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), 1e-3)\n",
    "\n",
    "def start_training(epochs, weights, scheduler, st_epoch=1, min_loss=10.0, max_acc=0.):\n",
    "    fn_epoch = st_epoch+epochs-1\n",
    "    len_fn_epoch = len(str(fn_epoch))\n",
    "    # loop over the dataset multiple times\n",
    "    for e in range(epochs):\n",
    "        cr_epoch = st_epoch+e\n",
    "        # training\n",
    "        data_num, running_loss, running_acc = 0, 0., 0.\n",
    "        model.train()\n",
    "        \n",
    "        pbar = tqdm(train_data, unit='batch',\n",
    "                    desc=f'{cr_epoch:0>{len_fn_epoch}}/{fn_epoch} - train: ', colour='yellow')\n",
    "        for batch in pbar:\n",
    "            # the input data is a list of [images, labels]\n",
    "            images, labels = batch\n",
    "            data_num += images.size(0)\n",
    "            \n",
    "            # clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, preds = torch.max(outputs.data, 1)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculate loss\n",
    "            running_loss += loss.item()*images.size(0)\n",
    "            running_acc += torch.sum(preds == labels).item()\n",
    "            \n",
    "            train_loss = running_loss/data_num\n",
    "            train_acc = running_acc/data_num\n",
    "            pbar.set_postfix_str(f'loss: {train_loss:.4f}, acc: {train_acc:.4f}')\n",
    "            \n",
    "        writer.add_scalar('Loss/Training', train_loss, cr_epoch)\n",
    "        writer.add_scalar('Accuracy/Training', train_acc, cr_epoch)\n",
    "        \n",
    "        # validation\n",
    "        data_num, running_loss, running_acc = 0, 0., 0.\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(valid_data, unit='batch',\n",
    "                        desc=f'{cr_epoch:0>{len_fn_epoch}}/{fn_epoch} - valid: ', colour='green')\n",
    "            for batch in pbar:\n",
    "                images, labels = batch\n",
    "                data_num += images.size(0)\n",
    "\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                _, preds = torch.max(outputs.data, 1)\n",
    "                \n",
    "                running_loss += loss.item()*images.size(0)\n",
    "                running_acc += torch.sum(preds == labels).item()\n",
    "                \n",
    "                valid_loss = running_loss/data_num\n",
    "                valid_acc = running_acc/data_num\n",
    "                pbar.set_postfix_str(f'loss: {valid_loss:.4f}, acc: {valid_acc:.4f}')\n",
    "\n",
    "        writer.add_scalar('Loss/Validation', valid_loss, cr_epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', valid_acc, cr_epoch)\n",
    "        \n",
    "        # updating scheduler each epoch\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Saving weights\n",
    "        if valid_loss < min_loss:\n",
    "            min_loss = valid_loss\n",
    "            torch.save(model.state_dict(), str(weights)+'best_loss.pth')\n",
    "        if valid_acc > max_acc:\n",
    "            max_acc = valid_acc\n",
    "            torch.save(model.state_dict(), str(weights)+'best_acc.pth')\n",
    "        torch.save(model.state_dict(), str(weights)+'last.pth')\n",
    "        \n",
    "    print(f\"{'='*20} Training finished. {'='*20}\")\n",
    "    print(f\"{'>'*10} Best loss: {min_loss:.4f}, Best acc: {max_acc:.4f} {'<'*10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "571956bd-930b-4827-b121-ece385d288cb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "001/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.54batch/s, loss: 1.2592, acc: 0.6268]\u001b[0m\n",
      "001/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:51<00:00,  8.09batch/s, loss: 0.7890, acc: 0.7729]\u001b[0m\n",
      "002/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:46<00:00,  2.55batch/s, loss: 0.7871, acc: 0.7667]\u001b[0m\n",
      "002/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.97batch/s, loss: 0.7271, acc: 0.7866]\u001b[0m\n",
      "003/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.54batch/s, loss: 0.6631, acc: 0.8037]\u001b[0m\n",
      "003/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.99batch/s, loss: 0.5868, acc: 0.8254]\u001b[0m\n",
      "004/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [14:14<00:00,  2.47batch/s, loss: 0.5828, acc: 0.8257]\u001b[0m\n",
      "004/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.94batch/s, loss: 0.5714, acc: 0.8326]\u001b[0m\n",
      "005/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:46<00:00,  2.55batch/s, loss: 0.5189, acc: 0.8437]\u001b[0m\n",
      "005/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:52<00:00,  8.01batch/s, loss: 0.5947, acc: 0.8311]\u001b[0m\n",
      "006/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:47<00:00,  2.55batch/s, loss: 0.4666, acc: 0.8590]\u001b[0m\n",
      "006/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:51<00:00,  8.09batch/s, loss: 0.5245, acc: 0.8524]\u001b[0m\n",
      "007/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:52<00:00,  2.53batch/s, loss: 0.4311, acc: 0.8674]\u001b[0m\n",
      "007/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:52<00:00,  8.06batch/s, loss: 0.5353, acc: 0.8452]\u001b[0m\n",
      "008/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:43<00:00,  2.56batch/s, loss: 0.3889, acc: 0.8805]\u001b[0m\n",
      "008/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:51<00:00,  8.09batch/s, loss: 0.4912, acc: 0.8590]\u001b[0m\n",
      "009/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:46<00:00,  2.55batch/s, loss: 0.3605, acc: 0.8867]\u001b[0m\n",
      "009/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.99batch/s, loss: 0.5159, acc: 0.8566]\u001b[0m\n",
      "010/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:47<00:00,  2.55batch/s, loss: 0.3266, acc: 0.8971]\u001b[0m\n",
      "010/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.98batch/s, loss: 0.5549, acc: 0.8528]\u001b[0m\n",
      "011/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:47<00:00,  2.55batch/s, loss: 0.3079, acc: 0.9015]\u001b[0m\n",
      "011/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:52<00:00,  8.02batch/s, loss: 0.5149, acc: 0.8581]\u001b[0m\n",
      "012/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:46<00:00,  2.55batch/s, loss: 0.2760, acc: 0.9120]\u001b[0m\n",
      "012/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.99batch/s, loss: 0.5043, acc: 0.8683]\u001b[0m\n",
      "013/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:41<00:00,  2.56batch/s, loss: 0.2611, acc: 0.9165]\u001b[0m\n",
      "013/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.94batch/s, loss: 0.5225, acc: 0.8672]\u001b[0m\n",
      "014/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.54batch/s, loss: 0.2477, acc: 0.9211]\u001b[0m\n",
      "014/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.93batch/s, loss: 0.5441, acc: 0.8594]\u001b[0m\n",
      "015/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:47<00:00,  2.55batch/s, loss: 0.2268, acc: 0.9278]\u001b[0m\n",
      "015/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.94batch/s, loss: 0.5272, acc: 0.8653]\u001b[0m\n",
      "016/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:46<00:00,  2.55batch/s, loss: 0.2187, acc: 0.9287]\u001b[0m\n",
      "016/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.91batch/s, loss: 0.5512, acc: 0.8651]\u001b[0m\n",
      "017/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:47<00:00,  2.55batch/s, loss: 0.2009, acc: 0.9342]\u001b[0m\n",
      "017/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:52<00:00,  8.02batch/s, loss: 0.5351, acc: 0.8645]\u001b[0m\n",
      "018/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:46<00:00,  2.55batch/s, loss: 0.1909, acc: 0.9379]\u001b[0m\n",
      "018/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:52<00:00,  8.01batch/s, loss: 0.5351, acc: 0.8714]\u001b[0m\n",
      "019/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:47<00:00,  2.55batch/s, loss: 0.1826, acc: 0.9400]\u001b[0m\n",
      "019/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  8.00batch/s, loss: 0.5430, acc: 0.8692]\u001b[0m\n",
      "020/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:47<00:00,  2.55batch/s, loss: 0.1716, acc: 0.9429]\u001b[0m\n",
      "020/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  8.01batch/s, loss: 0.5465, acc: 0.8723]\u001b[0m\n",
      "021/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:47<00:00,  2.55batch/s, loss: 0.1603, acc: 0.9470]\u001b[0m\n",
      "021/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  8.00batch/s, loss: 0.5806, acc: 0.8669]\u001b[0m\n",
      "022/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:47<00:00,  2.55batch/s, loss: 0.1583, acc: 0.9476]\u001b[0m\n",
      "022/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:52<00:00,  8.01batch/s, loss: 0.5826, acc: 0.8672]\u001b[0m\n",
      "023/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:46<00:00,  2.55batch/s, loss: 0.1456, acc: 0.9529]\u001b[0m\n",
      "023/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:52<00:00,  8.01batch/s, loss: 0.5609, acc: 0.8693]\u001b[0m\n",
      "024/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:47<00:00,  2.55batch/s, loss: 0.1413, acc: 0.9539]\u001b[0m\n",
      "024/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.96batch/s, loss: 0.5787, acc: 0.8704]\u001b[0m\n",
      "025/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.54batch/s, loss: 0.1368, acc: 0.9556]\u001b[0m\n",
      "025/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.98batch/s, loss: 0.5755, acc: 0.8692]\u001b[0m\n",
      "026/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:47<00:00,  2.55batch/s, loss: 0.1281, acc: 0.9581]\u001b[0m\n",
      "026/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.96batch/s, loss: 0.6170, acc: 0.8643]\u001b[0m\n",
      "027/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.55batch/s, loss: 0.1305, acc: 0.9557]\u001b[0m\n",
      "027/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.99batch/s, loss: 0.5839, acc: 0.8700]\u001b[0m\n",
      "028/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.54batch/s, loss: 0.1218, acc: 0.9606]\u001b[0m\n",
      "028/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.97batch/s, loss: 0.6115, acc: 0.8664]\u001b[0m\n",
      "029/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.55batch/s, loss: 0.1176, acc: 0.9611]\u001b[0m\n",
      "029/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.97batch/s, loss: 0.6362, acc: 0.8712]\u001b[0m\n",
      "030/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.54batch/s, loss: 0.1163, acc: 0.9614]\u001b[0m\n",
      "030/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.95batch/s, loss: 0.6229, acc: 0.8694]\u001b[0m\n",
      "031/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.54batch/s, loss: 0.1109, acc: 0.9625]\u001b[0m\n",
      "031/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.97batch/s, loss: 0.5933, acc: 0.8748]\u001b[0m\n",
      "032/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.54batch/s, loss: 0.1066, acc: 0.9652]\u001b[0m\n",
      "032/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.98batch/s, loss: 0.6334, acc: 0.8702]\u001b[0m\n",
      "033/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.54batch/s, loss: 0.1047, acc: 0.9661]\u001b[0m\n",
      "033/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.98batch/s, loss: 0.6498, acc: 0.8734]\u001b[0m\n",
      "034/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.54batch/s, loss: 0.1005, acc: 0.9673]\u001b[0m\n",
      "034/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.98batch/s, loss: 0.6243, acc: 0.8722]\u001b[0m\n",
      "035/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0986, acc: 0.9669]\u001b[0m\n",
      "035/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.97batch/s, loss: 0.6493, acc: 0.8696]\u001b[0m\n",
      "036/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0974, acc: 0.9677]\u001b[0m\n",
      "036/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.99batch/s, loss: 0.6241, acc: 0.8699]\u001b[0m\n",
      "037/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0932, acc: 0.9695]\u001b[0m\n",
      "037/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.96batch/s, loss: 0.6186, acc: 0.8746]\u001b[0m\n",
      "038/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0885, acc: 0.9706]\u001b[0m\n",
      "038/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.96batch/s, loss: 0.6239, acc: 0.8704]\u001b[0m\n",
      "039/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0873, acc: 0.9704]\u001b[0m\n",
      "039/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.97batch/s, loss: 0.6382, acc: 0.8718]\u001b[0m\n",
      "040/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0863, acc: 0.9718]\u001b[0m\n",
      "040/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.97batch/s, loss: 0.6591, acc: 0.8725]\u001b[0m\n",
      "041/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0814, acc: 0.9740]\u001b[0m\n",
      "041/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.95batch/s, loss: 0.6673, acc: 0.8716]\u001b[0m\n",
      "042/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0842, acc: 0.9725]\u001b[0m\n",
      "042/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.97batch/s, loss: 0.6900, acc: 0.8643]\u001b[0m\n",
      "043/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:56<00:00,  2.52batch/s, loss: 0.0809, acc: 0.9726]\u001b[0m\n",
      "043/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [02:02<00:00,  7.37batch/s, loss: 0.6500, acc: 0.8732]\u001b[0m\n",
      "044/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [14:21<00:00,  2.45batch/s, loss: 0.0802, acc: 0.9741]\u001b[0m\n",
      "044/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.92batch/s, loss: 0.6696, acc: 0.8697]\u001b[0m\n",
      "045/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:52<00:00,  2.53batch/s, loss: 0.0767, acc: 0.9744]\u001b[0m\n",
      "045/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.94batch/s, loss: 0.6867, acc: 0.8700]\u001b[0m\n",
      "046/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:50<00:00,  2.54batch/s, loss: 0.0750, acc: 0.9753]\u001b[0m\n",
      "046/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.96batch/s, loss: 0.6762, acc: 0.8705]\u001b[0m\n",
      "047/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0747, acc: 0.9754]\u001b[0m\n",
      "047/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.96batch/s, loss: 0.7135, acc: 0.8692]\u001b[0m\n",
      "048/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:50<00:00,  2.54batch/s, loss: 0.0754, acc: 0.9751]\u001b[0m\n",
      "048/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.94batch/s, loss: 0.6695, acc: 0.8756]\u001b[0m\n",
      "049/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:50<00:00,  2.54batch/s, loss: 0.0695, acc: 0.9772]\u001b[0m\n",
      "049/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.93batch/s, loss: 0.6647, acc: 0.8725]\u001b[0m\n",
      "050/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:50<00:00,  2.54batch/s, loss: 0.0708, acc: 0.9767]\u001b[0m\n",
      "050/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.96batch/s, loss: 0.7189, acc: 0.8736]\u001b[0m\n",
      "051/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0693, acc: 0.9769]\u001b[0m\n",
      "051/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.97batch/s, loss: 0.6638, acc: 0.8725]\u001b[0m\n",
      "052/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0655, acc: 0.9786]\u001b[0m\n",
      "052/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.95batch/s, loss: 0.7006, acc: 0.8752]\u001b[0m\n",
      "053/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0653, acc: 0.9779]\u001b[0m\n",
      "053/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.97batch/s, loss: 0.6945, acc: 0.8716]\u001b[0m\n",
      "054/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0650, acc: 0.9785]\u001b[0m\n",
      "054/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.95batch/s, loss: 0.7180, acc: 0.8714]\u001b[0m\n",
      "055/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0664, acc: 0.9782]\u001b[0m\n",
      "055/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.94batch/s, loss: 0.6719, acc: 0.8755]\u001b[0m\n",
      "056/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:50<00:00,  2.54batch/s, loss: 0.0627, acc: 0.9793]\u001b[0m\n",
      "056/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.95batch/s, loss: 0.7272, acc: 0.8731]\u001b[0m\n",
      "057/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0623, acc: 0.9794]\u001b[0m\n",
      "057/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.95batch/s, loss: 0.6709, acc: 0.8757]\u001b[0m\n",
      "058/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0589, acc: 0.9801]\u001b[0m\n",
      "058/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.95batch/s, loss: 0.7348, acc: 0.8735]\u001b[0m\n",
      "059/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0597, acc: 0.9811]\u001b[0m\n",
      "059/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.94batch/s, loss: 0.7083, acc: 0.8723]\u001b[0m\n",
      "060/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0571, acc: 0.9817]\u001b[0m\n",
      "060/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.94batch/s, loss: 0.7476, acc: 0.8695]\u001b[0m\n",
      "061/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:50<00:00,  2.54batch/s, loss: 0.0303, acc: 0.9902]\u001b[0m\n",
      "061/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.94batch/s, loss: 0.6490, acc: 0.8870]\u001b[0m\n",
      "062/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:50<00:00,  2.54batch/s, loss: 0.0157, acc: 0.9953]\u001b[0m\n",
      "062/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.96batch/s, loss: 0.6473, acc: 0.8882]\u001b[0m\n",
      "063/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0118, acc: 0.9963]\u001b[0m\n",
      "063/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.95batch/s, loss: 0.6554, acc: 0.8895]\u001b[0m\n",
      "064/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0101, acc: 0.9969]\u001b[0m\n",
      "064/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.94batch/s, loss: 0.6634, acc: 0.8898]\u001b[0m\n",
      "065/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.54batch/s, loss: 0.0080, acc: 0.9975]\u001b[0m\n",
      "065/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.96batch/s, loss: 0.6743, acc: 0.8903]\u001b[0m\n",
      "066/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0073, acc: 0.9980]\u001b[0m\n",
      "066/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.96batch/s, loss: 0.6814, acc: 0.8923]\u001b[0m\n",
      "067/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0068, acc: 0.9979]\u001b[0m\n",
      "067/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.95batch/s, loss: 0.6870, acc: 0.8909]\u001b[0m\n",
      "068/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.54batch/s, loss: 0.0059, acc: 0.9982]\u001b[0m\n",
      "068/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.97batch/s, loss: 0.6929, acc: 0.8913]\u001b[0m\n",
      "069/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0046, acc: 0.9986]\u001b[0m\n",
      "069/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.96batch/s, loss: 0.7084, acc: 0.8920]\u001b[0m\n",
      "070/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0050, acc: 0.9985]\u001b[0m\n",
      "070/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.94batch/s, loss: 0.7140, acc: 0.8912]\u001b[0m\n",
      "071/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0047, acc: 0.9987]\u001b[0m\n",
      "071/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.96batch/s, loss: 0.7108, acc: 0.8919]\u001b[0m\n",
      "072/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.54batch/s, loss: 0.0044, acc: 0.9987]\u001b[0m\n",
      "072/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.94batch/s, loss: 0.7111, acc: 0.8924]\u001b[0m\n",
      "073/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.54batch/s, loss: 0.0044, acc: 0.9985]\u001b[0m\n",
      "073/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.94batch/s, loss: 0.7276, acc: 0.8928]\u001b[0m\n",
      "074/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0040, acc: 0.9988]\u001b[0m\n",
      "074/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.95batch/s, loss: 0.7220, acc: 0.8937]\u001b[0m\n",
      "075/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0038, acc: 0.9988]\u001b[0m\n",
      "075/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.94batch/s, loss: 0.7357, acc: 0.8940]\u001b[0m\n",
      "076/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:50<00:00,  2.54batch/s, loss: 0.0038, acc: 0.9989]\u001b[0m\n",
      "076/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.93batch/s, loss: 0.7425, acc: 0.8927]\u001b[0m\n",
      "077/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:50<00:00,  2.54batch/s, loss: 0.0034, acc: 0.9991]\u001b[0m\n",
      "077/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.93batch/s, loss: 0.7500, acc: 0.8937]\u001b[0m\n",
      "078/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:50<00:00,  2.54batch/s, loss: 0.0032, acc: 0.9988]\u001b[0m\n",
      "078/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.92batch/s, loss: 0.7541, acc: 0.8936]\u001b[0m\n",
      "079/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0027, acc: 0.9993]\u001b[0m\n",
      "079/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.94batch/s, loss: 0.7430, acc: 0.8947]\u001b[0m\n",
      "080/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:48<00:00,  2.54batch/s, loss: 0.0030, acc: 0.9992]\u001b[0m\n",
      "080/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.94batch/s, loss: 0.7613, acc: 0.8936]\u001b[0m\n",
      "081/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0027, acc: 0.9992]\u001b[0m\n",
      "081/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.93batch/s, loss: 0.7543, acc: 0.8946]\u001b[0m\n",
      "082/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0025, acc: 0.9992]\u001b[0m\n",
      "082/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.93batch/s, loss: 0.7540, acc: 0.8955]\u001b[0m\n",
      "083/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0027, acc: 0.9992]\u001b[0m\n",
      "083/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.93batch/s, loss: 0.7464, acc: 0.8959]\u001b[0m\n",
      "084/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0024, acc: 0.9993]\u001b[0m\n",
      "084/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.94batch/s, loss: 0.7452, acc: 0.8947]\u001b[0m\n",
      "085/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0021, acc: 0.9994]\u001b[0m\n",
      "085/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.93batch/s, loss: 0.7497, acc: 0.8947]\u001b[0m\n",
      "086/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0024, acc: 0.9994]\u001b[0m\n",
      "086/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.92batch/s, loss: 0.7457, acc: 0.8949]\u001b[0m\n",
      "087/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:51<00:00,  2.54batch/s, loss: 0.0023, acc: 0.9994]\u001b[0m\n",
      "087/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [02:00<00:00,  7.50batch/s, loss: 0.7494, acc: 0.8949]\u001b[0m\n",
      "088/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [14:32<00:00,  2.42batch/s, loss: 0.0021, acc: 0.9994]\u001b[0m\n",
      "088/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:58<00:00,  7.65batch/s, loss: 0.7450, acc: 0.8958]\u001b[0m\n",
      "089/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [14:23<00:00,  2.44batch/s, loss: 0.0021, acc: 0.9995]\u001b[0m\n",
      "089/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:58<00:00,  7.63batch/s, loss: 0.7480, acc: 0.8964]\u001b[0m\n",
      "090/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [14:27<00:00,  2.43batch/s, loss: 0.0021, acc: 0.9995]\u001b[0m\n",
      "090/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:57<00:00,  7.71batch/s, loss: 0.7456, acc: 0.8967]\u001b[0m\n",
      "091/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [14:05<00:00,  2.49batch/s, loss: 0.0019, acc: 0.9995]\u001b[0m\n",
      "091/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:58<00:00,  7.62batch/s, loss: 0.7446, acc: 0.8951]\u001b[0m\n",
      "092/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [14:22<00:00,  2.44batch/s, loss: 0.0017, acc: 0.9995]\u001b[0m\n",
      "092/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.88batch/s, loss: 0.7488, acc: 0.8958]\u001b[0m\n",
      "093/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0020, acc: 0.9994]\u001b[0m\n",
      "093/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:52<00:00,  8.02batch/s, loss: 0.7449, acc: 0.8958]\u001b[0m\n",
      "094/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:50<00:00,  2.54batch/s, loss: 0.0019, acc: 0.9994]\u001b[0m\n",
      "094/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.93batch/s, loss: 0.7410, acc: 0.8963]\u001b[0m\n",
      "095/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:50<00:00,  2.54batch/s, loss: 0.0016, acc: 0.9995]\u001b[0m\n",
      "095/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:54<00:00,  7.94batch/s, loss: 0.7549, acc: 0.8956]\u001b[0m\n",
      "096/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0015, acc: 0.9996]\u001b[0m\n",
      "096/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.98batch/s, loss: 0.7511, acc: 0.8958]\u001b[0m\n",
      "097/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:50<00:00,  2.54batch/s, loss: 0.0013, acc: 0.9997]\u001b[0m\n",
      "097/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.97batch/s, loss: 0.7550, acc: 0.8962]\u001b[0m\n",
      "098/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:50<00:00,  2.54batch/s, loss: 0.0015, acc: 0.9996]\u001b[0m\n",
      "098/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.98batch/s, loss: 0.7558, acc: 0.8956]\u001b[0m\n",
      "099/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:52<00:00,  2.53batch/s, loss: 0.0015, acc: 0.9996]\u001b[0m\n",
      "099/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:53<00:00,  7.99batch/s, loss: 0.7542, acc: 0.8966]\u001b[0m\n",
      "100/100 - train: 100%|\u001b[33m███████████████████████████████\u001b[0m| 2108/2108 [13:49<00:00,  2.54batch/s, loss: 0.0015, acc: 0.9996]\u001b[0m\n",
      "100/100 - valid: 100%|\u001b[32m█████████████████████████████████\u001b[0m| 905/905 [01:52<00:00,  8.02batch/s, loss: 0.7520, acc: 0.8966]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Training finished. ====================\n",
      ">>>>>>>>>> Best loss: 0.4912, Best acc: 0.8967 <<<<<<<<<<\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "scheduler_step = [int(epochs*0.6), int(epochs*0.8)]\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, scheduler_step, gamma=0.1) # decreasing lr to lr*0.1\n",
    "weights = Path('weights', f'{model_name}_weights-')\n",
    "\n",
    "start_training(epochs, weights, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "172be185-9227-4042-b8f3-60bc2800571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for state in ['best_loss', 'best_acc', 'last']:\n",
    "    # load weights\n",
    "    weights_suffix = state+'.pth'\n",
    "    model.load_state_dict(torch.load(str(weights)+weights_suffix))\n",
    "    # save model\n",
    "    model_path = Path('models', f'{model_name}-'+weights_suffix)\n",
    "    torch.save(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95921c99-7024-4773-9401-cca24dd9d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "valid: 100%|███████████████████████████████████████████| 905/905 [01:54<00:00,  7.91batch/s, loss: 0.7456, acc: 0.8967]\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "model.load_state_dict(torch.load(str(weights)+'best_acc.pth')) # ['best_loss', 'best_acc', 'last']\n",
    "\n",
    "labels_lst, preds_lst = [], []\n",
    "data_num, running_loss, running_acc = 0, 0., 0.\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(valid_data, unit='batch', desc='valid: ')\n",
    "    for batch in pbar:\n",
    "        images, labels = batch\n",
    "        data_num += images.size(0)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        \n",
    "        running_loss += loss.item()*images.size(0)\n",
    "        running_acc += torch.sum(preds == labels).item()\n",
    "        \n",
    "        valid_loss = running_loss/data_num\n",
    "        valid_acc = running_acc/data_num\n",
    "        pbar.set_postfix_str(f'loss: {valid_loss:.4f}, acc: {valid_acc:.4f}')\n",
    "        \n",
    "        labels_lst.append(labels.cpu().tolist())\n",
    "        preds_lst.append(preds.cpu().tolist())\n",
    "\n",
    "labels_lst = [l for lst in labels_lst for l in lst]\n",
    "preds_lst = [l for lst in preds_lst for l in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "549a3c00-cad2-42e8-ac29-f84d2e8dbabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score(cfmat):\n",
    "    total_TP = 0\n",
    "    WP = 0\n",
    "    cls_num = cfmat.shape[0]\n",
    "    all_num = np.sum(cfmat)\n",
    "    cls_score = []\n",
    "    \n",
    "    for i in range(cls_num):\n",
    "        TP = cfmat[i, i]\n",
    "        FN = np.sum(cfmat[i, :]) - TP\n",
    "        FP = np.sum(cfmat[:, i]) - TP\n",
    "        TN = all_num - FN - FP + TP\n",
    "\n",
    "        TPR = TP / (TP+FN) if TP != 0 else 0 # recall (sensitivity)\n",
    "        PPV = TP / (TP+FP) if TP != 0 else 0 # precision\n",
    "        F1 = 2 * (PPV*TPR) / (PPV+TPR) if 0 not in [PPV, TPR] else 0 # F1-score\n",
    "        \n",
    "        total_TP += TP\n",
    "        WP += PPV * (TP+FN) # weighted precision\n",
    "        \n",
    "        cls_score.append({'TP': TP, 'FN': FN,\n",
    "                        'FP': FP, 'TN': TN,\n",
    "                        'TPR': round(TPR, 4),\n",
    "                        'PPV': round(PPV, 4),\n",
    "                        'F1': round(F1, 4)})\n",
    "    \n",
    "    ACC = total_TP / all_num\n",
    "    WP /= all_num\n",
    "    \n",
    "    print('{:<19} {:.4f}'.format('Accuracy:', ACC))\n",
    "    print('{:<19} {:.4f}'.format('Weighted-Precision:', WP))\n",
    "\n",
    "    return cls_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0836246-249e-4089-ad92-0b50a79e03ca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:           0.8967\n",
      "Weighted-Precision: 0.8967\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TP</th>\n",
       "      <th>FN</th>\n",
       "      <th>FP</th>\n",
       "      <th>TN</th>\n",
       "      <th>TPR</th>\n",
       "      <th>PPV</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>559</td>\n",
       "      <td>37</td>\n",
       "      <td>32</td>\n",
       "      <td>25809</td>\n",
       "      <td>0.9379</td>\n",
       "      <td>0.9459</td>\n",
       "      <td>0.9419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>810</td>\n",
       "      <td>103</td>\n",
       "      <td>106</td>\n",
       "      <td>25920</td>\n",
       "      <td>0.8872</td>\n",
       "      <td>0.8843</td>\n",
       "      <td>0.8857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>732</td>\n",
       "      <td>150</td>\n",
       "      <td>147</td>\n",
       "      <td>25754</td>\n",
       "      <td>0.8299</td>\n",
       "      <td>0.8328</td>\n",
       "      <td>0.8313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>798</td>\n",
       "      <td>113</td>\n",
       "      <td>173</td>\n",
       "      <td>25831</td>\n",
       "      <td>0.8760</td>\n",
       "      <td>0.8218</td>\n",
       "      <td>0.8480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>434</td>\n",
       "      <td>154</td>\n",
       "      <td>135</td>\n",
       "      <td>25464</td>\n",
       "      <td>0.7381</td>\n",
       "      <td>0.7627</td>\n",
       "      <td>0.7502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>499</td>\n",
       "      <td>77</td>\n",
       "      <td>44</td>\n",
       "      <td>25697</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.9190</td>\n",
       "      <td>0.8919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>620</td>\n",
       "      <td>47</td>\n",
       "      <td>24</td>\n",
       "      <td>25868</td>\n",
       "      <td>0.9295</td>\n",
       "      <td>0.9627</td>\n",
       "      <td>0.9458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>657</td>\n",
       "      <td>43</td>\n",
       "      <td>35</td>\n",
       "      <td>25898</td>\n",
       "      <td>0.9386</td>\n",
       "      <td>0.9494</td>\n",
       "      <td>0.9440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>889</td>\n",
       "      <td>50</td>\n",
       "      <td>44</td>\n",
       "      <td>26114</td>\n",
       "      <td>0.9468</td>\n",
       "      <td>0.9528</td>\n",
       "      <td>0.9498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>546</td>\n",
       "      <td>50</td>\n",
       "      <td>91</td>\n",
       "      <td>25724</td>\n",
       "      <td>0.9161</td>\n",
       "      <td>0.8571</td>\n",
       "      <td>0.8856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>798</td>\n",
       "      <td>103</td>\n",
       "      <td>114</td>\n",
       "      <td>25900</td>\n",
       "      <td>0.8857</td>\n",
       "      <td>0.8750</td>\n",
       "      <td>0.8803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>813</td>\n",
       "      <td>107</td>\n",
       "      <td>72</td>\n",
       "      <td>25953</td>\n",
       "      <td>0.8837</td>\n",
       "      <td>0.9186</td>\n",
       "      <td>0.9008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>647</td>\n",
       "      <td>45</td>\n",
       "      <td>57</td>\n",
       "      <td>25864</td>\n",
       "      <td>0.9350</td>\n",
       "      <td>0.9190</td>\n",
       "      <td>0.9269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>598</td>\n",
       "      <td>74</td>\n",
       "      <td>84</td>\n",
       "      <td>25759</td>\n",
       "      <td>0.8899</td>\n",
       "      <td>0.8768</td>\n",
       "      <td>0.8833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>590</td>\n",
       "      <td>69</td>\n",
       "      <td>93</td>\n",
       "      <td>25747</td>\n",
       "      <td>0.8953</td>\n",
       "      <td>0.8638</td>\n",
       "      <td>0.8793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>548</td>\n",
       "      <td>103</td>\n",
       "      <td>120</td>\n",
       "      <td>25644</td>\n",
       "      <td>0.8418</td>\n",
       "      <td>0.8204</td>\n",
       "      <td>0.8309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>557</td>\n",
       "      <td>109</td>\n",
       "      <td>89</td>\n",
       "      <td>25678</td>\n",
       "      <td>0.8363</td>\n",
       "      <td>0.8622</td>\n",
       "      <td>0.8491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>733</td>\n",
       "      <td>154</td>\n",
       "      <td>155</td>\n",
       "      <td>25743</td>\n",
       "      <td>0.8264</td>\n",
       "      <td>0.8255</td>\n",
       "      <td>0.8259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>556</td>\n",
       "      <td>56</td>\n",
       "      <td>52</td>\n",
       "      <td>25767</td>\n",
       "      <td>0.9085</td>\n",
       "      <td>0.9145</td>\n",
       "      <td>0.9115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>352</td>\n",
       "      <td>216</td>\n",
       "      <td>178</td>\n",
       "      <td>25277</td>\n",
       "      <td>0.6197</td>\n",
       "      <td>0.6642</td>\n",
       "      <td>0.6412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>713</td>\n",
       "      <td>145</td>\n",
       "      <td>131</td>\n",
       "      <td>25756</td>\n",
       "      <td>0.8310</td>\n",
       "      <td>0.8448</td>\n",
       "      <td>0.8378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>588</td>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>25798</td>\n",
       "      <td>0.8950</td>\n",
       "      <td>0.9363</td>\n",
       "      <td>0.9152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>672</td>\n",
       "      <td>26</td>\n",
       "      <td>34</td>\n",
       "      <td>25931</td>\n",
       "      <td>0.9628</td>\n",
       "      <td>0.9518</td>\n",
       "      <td>0.9573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>660</td>\n",
       "      <td>39</td>\n",
       "      <td>58</td>\n",
       "      <td>25882</td>\n",
       "      <td>0.9442</td>\n",
       "      <td>0.9192</td>\n",
       "      <td>0.9315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>914</td>\n",
       "      <td>29</td>\n",
       "      <td>52</td>\n",
       "      <td>26152</td>\n",
       "      <td>0.9692</td>\n",
       "      <td>0.9462</td>\n",
       "      <td>0.9576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>587</td>\n",
       "      <td>58</td>\n",
       "      <td>43</td>\n",
       "      <td>25805</td>\n",
       "      <td>0.9101</td>\n",
       "      <td>0.9317</td>\n",
       "      <td>0.9208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>626</td>\n",
       "      <td>59</td>\n",
       "      <td>32</td>\n",
       "      <td>25854</td>\n",
       "      <td>0.9139</td>\n",
       "      <td>0.9514</td>\n",
       "      <td>0.9322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>868</td>\n",
       "      <td>47</td>\n",
       "      <td>40</td>\n",
       "      <td>26100</td>\n",
       "      <td>0.9486</td>\n",
       "      <td>0.9559</td>\n",
       "      <td>0.9523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>886</td>\n",
       "      <td>39</td>\n",
       "      <td>81</td>\n",
       "      <td>26085</td>\n",
       "      <td>0.9578</td>\n",
       "      <td>0.9162</td>\n",
       "      <td>0.9366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>767</td>\n",
       "      <td>124</td>\n",
       "      <td>149</td>\n",
       "      <td>25813</td>\n",
       "      <td>0.8608</td>\n",
       "      <td>0.8373</td>\n",
       "      <td>0.8489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>836</td>\n",
       "      <td>81</td>\n",
       "      <td>57</td>\n",
       "      <td>26017</td>\n",
       "      <td>0.9117</td>\n",
       "      <td>0.9362</td>\n",
       "      <td>0.9238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>932</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>26220</td>\n",
       "      <td>0.9883</td>\n",
       "      <td>0.9790</td>\n",
       "      <td>0.9836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>918</td>\n",
       "      <td>29</td>\n",
       "      <td>34</td>\n",
       "      <td>26174</td>\n",
       "      <td>0.9694</td>\n",
       "      <td>0.9643</td>\n",
       "      <td>0.9668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TP   FN   FP     TN     TPR     PPV      F1\n",
       "0   559   37   32  25809  0.9379  0.9459  0.9419\n",
       "1   810  103  106  25920  0.8872  0.8843  0.8857\n",
       "2   732  150  147  25754  0.8299  0.8328  0.8313\n",
       "3   798  113  173  25831  0.8760  0.8218  0.8480\n",
       "4   434  154  135  25464  0.7381  0.7627  0.7502\n",
       "5   499   77   44  25697  0.8663  0.9190  0.8919\n",
       "6   620   47   24  25868  0.9295  0.9627  0.9458\n",
       "7   657   43   35  25898  0.9386  0.9494  0.9440\n",
       "8   889   50   44  26114  0.9468  0.9528  0.9498\n",
       "9   546   50   91  25724  0.9161  0.8571  0.8856\n",
       "10  798  103  114  25900  0.8857  0.8750  0.8803\n",
       "11  813  107   72  25953  0.8837  0.9186  0.9008\n",
       "12  647   45   57  25864  0.9350  0.9190  0.9269\n",
       "13  598   74   84  25759  0.8899  0.8768  0.8833\n",
       "14  590   69   93  25747  0.8953  0.8638  0.8793\n",
       "15  548  103  120  25644  0.8418  0.8204  0.8309\n",
       "16  557  109   89  25678  0.8363  0.8622  0.8491\n",
       "17  733  154  155  25743  0.8264  0.8255  0.8259\n",
       "18  556   56   52  25767  0.9085  0.9145  0.9115\n",
       "19  352  216  178  25277  0.6197  0.6642  0.6412\n",
       "20  713  145  131  25756  0.8310  0.8448  0.8378\n",
       "21  588   69   40  25798  0.8950  0.9363  0.9152\n",
       "22  672   26   34  25931  0.9628  0.9518  0.9573\n",
       "23  660   39   58  25882  0.9442  0.9192  0.9315\n",
       "24  914   29   52  26152  0.9692  0.9462  0.9576\n",
       "25  587   58   43  25805  0.9101  0.9317  0.9208\n",
       "26  626   59   32  25854  0.9139  0.9514  0.9322\n",
       "27  868   47   40  26100  0.9486  0.9559  0.9523\n",
       "28  886   39   81  26085  0.9578  0.9162  0.9366\n",
       "29  767  124  149  25813  0.8608  0.8373  0.8489\n",
       "30  836   81   57  26017  0.9117  0.9362  0.9238\n",
       "31  932   11   20  26220  0.9883  0.9790  0.9836\n",
       "32  918   29   34  26174  0.9694  0.9643  0.9668"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_mat = confusion_matrix(labels_lst, preds_lst, labels=list(range(33)))\n",
    "score = calc_score(confusion_mat)\n",
    "pd.DataFrame(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00981b2-3acf-479d-a458-4cb26b833781",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e1e4d-c731-4a35-843e-ae82de8d47d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-1-13",
   "language": "python",
   "name": "pytorch-1-13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
